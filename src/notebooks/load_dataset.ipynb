{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaders \n",
    "\n",
    "In this notebook we will use the dataloader. The dataloader is a class that will help us to load the data in batches. This is very useful when we have a lot of data and we don't want to load all the data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evivoli/miniconda3/envs/asmara/lib/python3.9/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: unknown is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from pathlib import Path\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from src.data.datasets.holo_data import LandmineDataset\n",
    "from src.models.nets.resnet import ResNet50\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 0\n",
      "data:\n",
      "  dataset: holograms\n",
      "  task: bin\n",
      "  remove: []\n",
      "  paths:\n",
      "    holograms: data/processed/holograms\n",
      "    inversions: data/processed/inversions\n",
      "    splits: data/processed/splits\n",
      "  sample_rate: 0.2\n",
      "  folds:\n",
      "    train: train\n",
      "    val: val\n",
      "    test: test\n",
      "  batch_size: 8\n",
      "model:\n",
      "  num_classes: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hydra context initialization\n",
    "with initialize(version_base=None, config_path=\"../models/configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\", overrides=[\"data.dataset=holograms\", \"data.task=bin\"])\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "assert cfg.data.dataset != None, \"Please specify a dataset in the config file. [holograms, inversion]\"\n",
    "assert cfg.data.task != None, \"Please specify a task in the config file. [bin, tri, fine-grain]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We recover the original path of the dataset:\n",
    "datapath = Path('/home/evivoli/asmara') / Path(cfg.data.paths[f'{cfg.data.dataset}'])\n",
    "metapath = Path('/home/evivoli/asmara') / Path(cfg.data.paths.splits)\n",
    "\n",
    "# Load data\n",
    "train_data = LandmineDataset(data_path=datapath, \n",
    "                            meta_path=metapath, \n",
    "                            sample_rate=cfg.data.sample_rate, \n",
    "                            fold=cfg.data.folds.train, \n",
    "                            remove=cfg.data.remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=cfg.data.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evivoli/asmara/src/data/datasets/holo_data.py:39: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:250.)\n",
      "  data = torch.from_numpy(data).double()\n",
      "/home/evivoli/asmara/src/data/datasets/holo_data.py:56: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3277.)\n",
      "  mix = torch.nan_to_num(mix).T\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 3 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m x, yb \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/asmara/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    161\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    162\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 3 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "for x, yb in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = train_data.__sizeof__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holgraphic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import task_manager, get_holo_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, metapath: Path = BASEPATH / Path('data/processed/meta'),\n",
    "                       hologramspath: Path = BASEPATH / Path('data/interim/holograms'),\n",
    "                       task: str = '2-class',\n",
    "                       only_real: bool = True, \n",
    "                       remove: List = []) -> None:\n",
    "\n",
    "        self.metapath = metapath\n",
    "        # tasks that can be conducted are:\n",
    "        # - '2-class'     - classification [mine / clutter]\n",
    "        # - '3-class'   - classification [mine / clutter / arch]\n",
    "        # - 'fg-class'  - fine-grain classification [vs50, wood-cylinder, ...]\n",
    "        self.task = task\n",
    "        self.only_real = only_real\n",
    "\n",
    "        self.indoorpath = hologramspath / Path('indoor')\n",
    "        self.outdoorpath = hologramspath / Path('outdoor')\n",
    "\n",
    "        self.csv = pd.read_csv(metapath / Path('mix.csv'))\n",
    "        # filter out some\n",
    "        self.csv = self.csv[~self.csv['in_id'].isin(remove)]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Returns (xb, yb) pair, after applying all transformations on the audio file.\n",
    "        row = self.csv.iloc[index]\n",
    "        in_holo = np.load(self.indoorpath / f\"{row['in_file_name']}.npy\")\n",
    "        in_holo = torch.from_numpy(in_holo).double()\n",
    "        \n",
    "        holo_noise = get_holo_noise(self.indoorpath, row['in_file_name'], row['in_id'], in_holo.shape)\n",
    "        holo_noise = torch.from_numpy(holo_noise).double()\n",
    "\n",
    "        out_holo = np.load(self.outdoorpath / f\"{row['out_file_name']}.npy\")\n",
    "        out_holo = torch.from_numpy(out_holo).double()\n",
    "\n",
    "        print(in_holo.shape , holo_noise.shape, out_holo.shape)\n",
    "        if out_holo.shape[-1] == 61:\n",
    "            return None, None\n",
    "        mix_holo =  ((in_holo - holo_noise) + out_holo)\n",
    "\n",
    "        tasks = ['2-class','3-class','fg-class']\n",
    "\n",
    "        if self.task in tasks:\n",
    "            label = task_manager(self.task, row)\n",
    "        else:\n",
    "            raise ValueError(f'task {self.task} is not supported !')\n",
    "        \n",
    "        mix = torch.real(mix_holo) if self.only_real else mix_holo\n",
    "        mix = mix.unsqueeze(0)\n",
    "        mix = torch.nan_to_num(mix).T\n",
    "        return mix.float(), label\n",
    "\n",
    "    def __sizeof__(self) -> int:\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = HoloDataset(task='fg-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for x, yb in train_data:\n",
    "    if count > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = train_data.__sizeof__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use folds 1,2,3 for training, 4 for validation, 5 for testing.\n",
    "test_data, val_data, train_data = random_split(HoloDataset(task='2-class'), [int(SIZE*0.1), int(SIZE*0.2), SIZE-(int(SIZE*0.1) + int(SIZE*0.2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = \\\n",
    "    torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(pl.LightningModule):\n",
    "    \"\"\"Pytorch Lightning implementation of U-Net.\n",
    "\n",
    "    Paper: `U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    <https://arxiv.org/abs/1505.04597>`_\n",
    "\n",
    "    Paper authors: Olaf Ronneberger, Philipp Fischer, Thomas Brox\n",
    "\n",
    "    Implemented by:\n",
    "\n",
    "        - `Annika Brundyn <https://github.com/annikabrundyn>`_\n",
    "        - `Akshay Kulkarni <https://github.com/akshaykvnit>`_\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes required\n",
    "        input_channels: Number of channels in input images (default 3)\n",
    "        num_layers: Number of layers in each side of U-net (default 5)\n",
    "        features_start: Number of features in first layer (default 64)\n",
    "        bilinear: Whether to use bilinear interpolation (True) or transposed convolutions (default) for upsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        input_channels: int = 3,\n",
    "        num_layers: int = 5,\n",
    "        features_start: int = 64,\n",
    "        bilinear: bool = False,\n",
    "    ):\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(f\"num_layers = {num_layers}, expected: num_layers > 0\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DoubleConv(input_channels, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2, bilinear))\n",
    "            feats //= 2\n",
    "\n",
    "        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        xi = [self.layers[0](x)]\n",
    "        # Down path\n",
    "        for layer in self.layers[1 : self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        # Up path\n",
    "        for i, layer in enumerate(self.layers[self.num_layers : -1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        return self.layers[-1](xi[-1])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Very simple training loop\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "        acc = functional.accuracy(y_hat, y)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return acc\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"[ Conv2d => BatchNorm => ReLU ] x 2.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscale with MaxPool => DoubleConvolution block.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), DoubleConv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling (by either bilinear interpolation or transpose convolutions) followed by concatenation of feature\n",
    "    map from contracting path, followed by DoubleConv.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = None\n",
    "        if bilinear:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the network works on a single mini-batch\n",
    "audionet = UNet(num_classes = 2, input_channels = 1)\n",
    "xb, yb = next(iter(train_loader))\n",
    "audionet(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=0, max_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(audionet, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to implement\n",
    "\n",
    "A list of models to implement are:\n",
    "- [3D U-NET++](https://github.com/wolny/pytorch-3dunet)\n",
    "- [Attention-UNet](https://github.com/ozan-oktay/Attention-Gated-Networks)\n",
    "- [FetalCPSeg](https://github.com/wulalago/FetalCPSeg) `DSRNet`\n",
    "- [SEFCN]()\n",
    "\n",
    "\n",
    "Then there is a repo with all\n",
    "- [Segmentation models](https://github.com/qubvel/segmentation_models.pytorch)\n",
    "\n",
    "With the following architetcures:\n",
    "\n",
    "- Unet \n",
    "- Unet++ \n",
    "- MAnet \n",
    "- Linknet \n",
    "- FPN \n",
    "- PSPNet\n",
    "- PAN \n",
    "- DeepLabV3 \n",
    "- DeepLabV3+ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asmara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:05:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ea0a4f467ddd1ee6892d2aa7c229289ebc223d4acc1e922b15dbf8f4e97eb4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
