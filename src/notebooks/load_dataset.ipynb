{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation \n",
    "\n",
    "In this npotebook we create the `Dataset` object from pytorch.\n",
    "\n",
    "The `Dataset` we want to create is based on metadata we have buolt, and is respobsible to load the two `indoor` and `outdoor` elements and mixing them on the fly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from src.utils.const import *\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, metapath: Path = BASEPATH / Path('data/processed/meta'),\n",
    "                       inversionspath: Path = BASEPATH / Path('data/interim/inversions'),\n",
    "                       task: str = '2-class',\n",
    "                       only_real: bool = True, \n",
    "                       remove: List = []) -> None:\n",
    "\n",
    "        self.metapath = metapath\n",
    "        # tasks that can be conducted are:\n",
    "        # - '2-class'     - classification [mine / clutter]\n",
    "        # - '3-class'   - classification [mine / clutter / arch]\n",
    "        # - 'fg-class'  - fine-grain classification [vs50, wood-cylinder, ...]\n",
    "        self.task = task\n",
    "        self.only_real = only_real\n",
    "\n",
    "        self.indoorpath = inversionspath / Path('indoor')\n",
    "        self.outdoorpath = inversionspath / Path('outdoor')\n",
    "\n",
    "        self.csv = pd.read_csv(metapath / Path('mix.csv'))\n",
    "        # filter out some\n",
    "        self.csv = self.csv[~self.csv['in_id'].isin(remove)]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Returns (xb, yb) pair, after applying all transformations on the audio file.\n",
    "        row = self.csv.iloc[index]\n",
    "        in_inv = np.load(self.indoorpath / f\"{row['in_file_name']}.npy\")\n",
    "        in_inv = torch.from_numpy(in_inv).double()\n",
    "        \n",
    "        try: inv_noise = np.load(self.indoorpath / f\"{row['in_file_name']}.npy\".replace(str(row['in_id']).zfill(2), '00'))\n",
    "        except:\n",
    "            try: inv_noise = np.load(self.indoorpath / f\"{row['in_file_name'].replace('bas', 'low')}.npy\".replace(str(row['in_id']).zfill(2), '00'))\n",
    "            except:\n",
    "                print(f\"no noise signal {row['in_file_name'].replace(str(row['in_id']).zfill(2), '00')} exists for {row['in_file_name']}\")\n",
    "                inv_noise = np.zeros_like(in_inv)\n",
    "        inv_noise = torch.from_numpy(inv_noise).double()\n",
    "\n",
    "        out_inv = np.load(self.outdoorpath / f\"{row['out_file_name']}.npy\")\n",
    "        out_inv = torch.from_numpy(out_inv).double()\n",
    "\n",
    "        mix_inv =  (in_inv - inv_noise) + out_inv\n",
    "\n",
    "        mix_inv = mix_inv.T\n",
    "\n",
    "    \n",
    "        def twoclass(label:str)->int:\n",
    "            # 1 -> mine\n",
    "            # 0 -> other\n",
    "            return 1 if label == 'mine' else 0\n",
    "\n",
    "        def threeclass(label:str)->int:\n",
    "            # 2 -> archeology \n",
    "            # 1 -> mine\n",
    "            # 0 -> other\n",
    "            return 1 if label == 'mine' else 2 if label == 'archeology' else 0\n",
    "\n",
    "        def fgclass(label:str)->int:\n",
    "            # 14 -> knife\n",
    "            # 13 -> terracotta\n",
    "            # ...\n",
    "            # 1 -> pmn1\n",
    "            # 0 -> none\n",
    "            return int(label)\n",
    "\n",
    "        if self.task == '2-class':\n",
    "            label = twoclass(row['in_name'])\n",
    "        elif self.task == '3-class':\n",
    "            label = threeclass(row['in_name'])\n",
    "        elif self.task == 'fg-class':\n",
    "            label = fgclass(row['in_id'])\n",
    "        else:\n",
    "            raise ValueError(f'task {self.task} is not supported !')\n",
    "        \n",
    "        mix = torch.real(mix_inv) if self.only_real else mix_inv\n",
    "        mix = torch.nan_to_num(mix)\n",
    "        return mix.float(), label\n",
    "\n",
    "    def __sizeof__(self) -> int:\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.csv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InvDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, yb in train_data:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = train_data.__sizeof__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holgraphic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data import task_manager, get_holo_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoloDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, metapath: Path = BASEPATH / Path('data/processed/meta'),\n",
    "                       hologramspath: Path = BASEPATH / Path('data/interim/holograms'),\n",
    "                       task: str = '2-class',\n",
    "                       only_real: bool = True, \n",
    "                       remove: List = []) -> None:\n",
    "\n",
    "        self.metapath = metapath\n",
    "        # tasks that can be conducted are:\n",
    "        # - '2-class'     - classification [mine / clutter]\n",
    "        # - '3-class'   - classification [mine / clutter / arch]\n",
    "        # - 'fg-class'  - fine-grain classification [vs50, wood-cylinder, ...]\n",
    "        self.task = task\n",
    "        self.only_real = only_real\n",
    "\n",
    "        self.indoorpath = hologramspath / Path('indoor')\n",
    "        self.outdoorpath = hologramspath / Path('outdoor')\n",
    "\n",
    "        self.csv = pd.read_csv(metapath / Path('mix.csv'))\n",
    "        # filter out some\n",
    "        self.csv = self.csv[~self.csv['in_id'].isin(remove)]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Returns (xb, yb) pair, after applying all transformations on the audio file.\n",
    "        row = self.csv.iloc[index]\n",
    "        in_holo = np.load(self.indoorpath / f\"{row['in_file_name']}.npy\")\n",
    "        in_holo = torch.from_numpy(in_holo).double()\n",
    "        \n",
    "        holo_noise = get_holo_noise(self.indoorpath, row['in_file_name'], row['in_id'], in_holo.shape)\n",
    "        holo_noise = torch.from_numpy(holo_noise).double()\n",
    "\n",
    "        out_holo = np.load(self.outdoorpath / f\"{row['out_file_name']}.npy\")\n",
    "        out_holo = torch.from_numpy(out_holo).double()\n",
    "\n",
    "        print(in_holo.shape , holo_noise.shape, out_holo.shape)\n",
    "        if out_holo.shape[-1] == 61:\n",
    "            return None, None\n",
    "        mix_holo =  ((in_holo - holo_noise) + out_holo)\n",
    "\n",
    "        tasks = ['2-class','3-class','fg-class']\n",
    "\n",
    "        if self.task in tasks:\n",
    "            label = task_manager(self.task, row)\n",
    "        else:\n",
    "            raise ValueError(f'task {self.task} is not supported !')\n",
    "        \n",
    "        mix = torch.real(mix_holo) if self.only_real else mix_holo\n",
    "        mix = mix.unsqueeze(0)\n",
    "        mix = torch.nan_to_num(mix).T\n",
    "        return mix.float(), label\n",
    "\n",
    "    def __sizeof__(self) -> int:\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = HoloDataset(task='fg-class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 62])\n",
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/emanuelevivoli/asmara/data/interim/holograms/outdoor/36_295_6.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, yb \u001b[39min\u001b[39;00m train_data:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m count \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m: \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 13\u001b[0m in \u001b[0;36mHoloDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m holo_noise \u001b[39m=\u001b[39m get_holo_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindoorpath, row[\u001b[39m'\u001b[39m\u001b[39min_file_name\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39min_id\u001b[39m\u001b[39m'\u001b[39m], in_holo\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m holo_noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(holo_noise)\u001b[39m.\u001b[39mdouble()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m out_holo \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutdoorpath \u001b[39m/\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mrow[\u001b[39m'\u001b[39;49m\u001b[39mout_file_name\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out_holo \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(out_holo)\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X41sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(in_holo\u001b[39m.\u001b[39mshape , holo_noise\u001b[39m.\u001b[39mshape, out_holo\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/emanuelevivoli/asmara/data/interim/holograms/outdoor/36_295_6.npy'"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x, yb in train_data:\n",
    "    if count > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = train_data.__sizeof__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use folds 1,2,3 for training, 4 for validation, 5 for testing.\n",
    "test_data, val_data, train_data = random_split(HoloDataset(task='2-class'), [int(SIZE*0.1), int(SIZE*0.2), SIZE-(int(SIZE*0.1) + int(SIZE*0.2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = \\\n",
    "    torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchmetrics import functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(pl.LightningModule):\n",
    "    \"\"\"Pytorch Lightning implementation of U-Net.\n",
    "\n",
    "    Paper: `U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "    <https://arxiv.org/abs/1505.04597>`_\n",
    "\n",
    "    Paper authors: Olaf Ronneberger, Philipp Fischer, Thomas Brox\n",
    "\n",
    "    Implemented by:\n",
    "\n",
    "        - `Annika Brundyn <https://github.com/annikabrundyn>`_\n",
    "        - `Akshay Kulkarni <https://github.com/akshaykvnit>`_\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes required\n",
    "        input_channels: Number of channels in input images (default 3)\n",
    "        num_layers: Number of layers in each side of U-net (default 5)\n",
    "        features_start: Number of features in first layer (default 64)\n",
    "        bilinear: Whether to use bilinear interpolation (True) or transposed convolutions (default) for upsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        input_channels: int = 3,\n",
    "        num_layers: int = 5,\n",
    "        features_start: int = 64,\n",
    "        bilinear: bool = False,\n",
    "    ):\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(f\"num_layers = {num_layers}, expected: num_layers > 0\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = [DoubleConv(input_channels, features_start)]\n",
    "\n",
    "        feats = features_start\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Down(feats, feats * 2))\n",
    "            feats *= 2\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(Up(feats, feats // 2, bilinear))\n",
    "            feats //= 2\n",
    "\n",
    "        layers.append(nn.Conv2d(feats, num_classes, kernel_size=1))\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        xi = [self.layers[0](x)]\n",
    "        # Down path\n",
    "        for layer in self.layers[1 : self.num_layers]:\n",
    "            xi.append(layer(xi[-1]))\n",
    "        # Up path\n",
    "        for i, layer in enumerate(self.layers[self.num_layers : -1]):\n",
    "            xi[-1] = layer(xi[-1], xi[-2 - i])\n",
    "        return self.layers[-1](xi[-1])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Very simple training loop\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.argmax(y_hat, dim=1)\n",
    "        acc = functional.accuracy(y_hat, y)\n",
    "        self.log('val_acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return acc\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"[ Conv2d => BatchNorm => ReLU ] x 2.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscale with MaxPool => DoubleConvolution block.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.MaxPool2d(kernel_size=2, stride=2), DoubleConv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upsampling (by either bilinear interpolation or transpose convolutions) followed by concatenation of feature\n",
    "    map from contracting path, followed by DoubleConv.\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch: int, out_ch: int, bilinear: bool = False):\n",
    "        super().__init__()\n",
    "        self.upsample = None\n",
    "        if bilinear:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "                nn.Conv2d(in_ch, in_ch // 2, kernel_size=1),\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
    "        x1 = self.upsample(x1)\n",
    "\n",
    "        # Pad x1 to the size of x2\n",
    "        diff_h = x2.shape[2] - x1.shape[2]\n",
    "        diff_w = x2.shape[3] - x1.shape[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diff_w // 2, diff_w - diff_w // 2, diff_h // 2, diff_h - diff_h // 2])\n",
    "\n",
    "        # Concatenate along the channels axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52, 62]) torch.Size([52, 62]) torch.Size([52, 61])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (62) must match the size of tensor b (61) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Test that the network works on a single mini-batch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m audionet \u001b[39m=\u001b[39m UNet(num_classes \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, input_channels \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m xb, yb \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_loader))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m audionet(xb)\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 25\u001b[0m in \u001b[0;36mHoloDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m out_holo \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(out_holo)\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(in_holo\u001b[39m.\u001b[39mshape , holo_noise\u001b[39m.\u001b[39mshape, out_holo\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m mix_holo \u001b[39m=\u001b[39m  ((in_holo \u001b[39m-\u001b[39;49m holo_noise) \u001b[39m+\u001b[39;49m out_holo)\u001b[39m.\u001b[39mT\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m tasks \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m2-class\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m3-class\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mfg-class\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39min\u001b[39;00m tasks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (62) must match the size of tensor b (61) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Test that the network works on a single mini-batch\n",
    "audionet = UNet(num_classes = 2, input_channels = 1)\n",
    "xb, yb = next(iter(train_loader))\n",
    "audionet(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fb3efd21700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=0, max_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /Users/emanuelevivoli/asmara/src/notebooks/lightning_logs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | ModuleList | 31.1 M\n",
      "--------------------------------------\n",
      "31.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.1 M    Total params\n",
      "124.238   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb8f1d841104feeb91cd530501a3263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(audionet, train_loader, val_loader)\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 696\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    698\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    651\u001b[0m \u001b[39m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    731\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    732\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    733\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    734\u001b[0m )\n\u001b[0;32m--> 735\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    737\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    738\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1166\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1168\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1273\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1276\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> 1343\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1345\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1347\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    154\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 155\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:143\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    142\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:240\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 240\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1704\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1701\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1704\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1706\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:370\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    369\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb Cell 19\u001b[0m in \u001b[0;36mUNet.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m y_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(y_hat, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m acc \u001b[39m=\u001b[39m functional\u001b[39m.\u001b[39;49maccuracy(y_hat, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m, acc, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emanuelevivoli/asmara/src/notebooks/load_dataset.ipynb#X30sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mreturn\u001b[39;00m acc\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py:815\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(preds, target, average, mdmc_average, threshold, top_k, subset_accuracy, num_classes, multiclass, ignore_index, task, num_labels, multidim_average, validate_args)\u001b[0m\n\u001b[1;32m    812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe `top_k` should be an integer larger than 0, got \u001b[39m\u001b[39m{\u001b[39;00mtop_k\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    814\u001b[0m preds, target \u001b[39m=\u001b[39m _input_squeeze(preds, target)\n\u001b[0;32m--> 815\u001b[0m mode \u001b[39m=\u001b[39m _mode(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\n\u001b[1;32m    816\u001b[0m reduce \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m average \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m] \u001b[39melse\u001b[39;00m average\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m subset_accuracy \u001b[39mand\u001b[39;00m _check_subset_validity(mode):\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torchmetrics/functional/classification/accuracy.py:423\u001b[0m, in \u001b[0;36m_mode\u001b[0;34m(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mode\u001b[39m(\n\u001b[1;32m    394\u001b[0m     preds: Tensor,\n\u001b[1;32m    395\u001b[0m     target: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     ignore_index: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataType:\n\u001b[1;32m    402\u001b[0m     \u001b[39m\"\"\"Finds the mode of the input tensors.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m        <DataType.MULTICLASS: 'multi-class'>\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     mode \u001b[39m=\u001b[39m _check_classification_inputs(\n\u001b[1;32m    424\u001b[0m         preds,\n\u001b[1;32m    425\u001b[0m         target,\n\u001b[1;32m    426\u001b[0m         threshold\u001b[39m=\u001b[39;49mthreshold,\n\u001b[1;32m    427\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    428\u001b[0m         num_classes\u001b[39m=\u001b[39;49mnum_classes,\n\u001b[1;32m    429\u001b[0m         multiclass\u001b[39m=\u001b[39;49mmulticlass,\n\u001b[1;32m    430\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mreturn\u001b[39;00m mode\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:273\u001b[0m, in \u001b[0;36m_check_classification_inputs\u001b[0;34m(preds, target, threshold, num_classes, multiclass, top_k, ignore_index)\u001b[0m\n\u001b[1;32m    270\u001b[0m _basic_input_validation(preds, target, threshold, multiclass, ignore_index)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Check that shape/types fall into one of the cases\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m case, implied_classes \u001b[39m=\u001b[39m _check_shape_and_type_consistency(preds, target)\n\u001b[1;32m    275\u001b[0m \u001b[39m# Check consistency with the `C` dimension in case of multi-class data\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m preds\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m target\u001b[39m.\u001b[39mshape:\n",
      "File \u001b[0;32m~/miniconda3/envs/mypython3/lib/python3.8/site-packages/torchmetrics/utilities/checks.py:119\u001b[0m, in \u001b[0;36m_check_shape_and_type_consistency\u001b[0;34m(preds, target)\u001b[0m\n\u001b[1;32m    117\u001b[0m         case \u001b[39m=\u001b[39m DataType\u001b[39m.\u001b[39mMULTIDIM_MULTICLASS\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEither `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m and `preds` should be (N, C, ...).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m     )\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m case, implied_classes\n",
      "\u001b[0;31mValueError\u001b[0m: Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...) and `preds` should be (N, C, ...)."
     ]
    }
   ],
   "source": [
    "trainer.fit(audionet, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to implement\n",
    "\n",
    "A list of models to implement are:\n",
    "- [3D U-NET++](https://github.com/wolny/pytorch-3dunet)\n",
    "- [Attention-UNet](https://github.com/ozan-oktay/Attention-Gated-Networks)\n",
    "- [FetalCPSeg](https://github.com/wulalago/FetalCPSeg) `DSRNet`\n",
    "- [SEFCN]()\n",
    "\n",
    "\n",
    "Then there is a repo with all\n",
    "- [Segmentation models](https://github.com/qubvel/segmentation_models.pytorch)\n",
    "\n",
    "With the following architetcures:\n",
    "\n",
    "- Unet \n",
    "- Unet++ \n",
    "- MAnet \n",
    "- Linknet \n",
    "- FPN \n",
    "- PSPNet\n",
    "- PAN \n",
    "- DeepLabV3 \n",
    "- DeepLabV3+ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mypython3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8148f3366ec21db64bd0484ee2e8d964d27d8e8b73627767738b51624036cdd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
